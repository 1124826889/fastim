## FastIM
> 🚀基于Netty的高扩展高可用分布式即时通讯系统，支持单聊、群聊、登录登出、聊天记录查询、离线消息存储、消息推送、心跳、分布式唯一ID、红包、消息漫游等功能。支持分布式部署并且基于MQ的可扩展性架构。

## 一、系统设计
### 1. 逻辑结构图和架构图
#### 1.0 架构图
基于可扩展性高可用原则，把网络层、业务逻辑层、数据层分离，网络层和业务层支持负载均衡策略、数据层支持分布式存储, 支持分布式部署.
![09-icRnjQ](https://github.com/zhangyaoo/fastim/blob/master/IM系统架构图.jpg)
#### 1.1 逻辑图


#### 1.0 CLIENT设计：
1. client每个设备会在本地存每一个会话，保留有最新一条消息的顺序 ID
2. 为了避免client宕机，也就是退出应用，保存在内存的消息ID丢失，会存到本地的文件中


#### 1.0 LSB设计：
1. 接入层的高可用、负载均衡、扩展性全部在这里面做
2. 客户端通过LSB，来获取gate IP地址，通过IP直连，目的是
    - 灵活的负载均衡策略 可根据最少连接数来分配IP
    - 做灰度策略来分配IP
    - AppId业务隔离策略 不同业务连接不同的gate，防止相互影响

#### 1.2 GATE设计：
1. 任何一个gate网关断掉，用户端检测到以后重新连接LSB服务获取另一个gate网关IP，拿到IP重新进行长连接通信。对整体服务可靠性基本没有影响。
2. gate可以无状态的横向部署，来扩展接入层的接入能力
3. 从性能的角度出发，gate层需要存储本地session，并且系统在变更session时候保证了本地session和分布式session的一致性

#### 1.3 ROUTER设计：
1、把用户状态信息存储在Redis集群里。因此也是无状态的，任何一个router服务挂掉，不影响整体服务能力。
2. 转发消息，将消息投递给固定gate上的会话
3. router层需要存储的关系
- uid和机器ID关系

#### 1.4 LOGIC设计：
logic按照分布式微服务的拆分思想进行拆分，拆分为多个模块
1. 单聊服务
2. 群聊服务
3. 登录 登出 注册服务
4. 红包服务
5. 分布式ID服务
6. 离线服务

#### 1.5 DAS (data access service)设计：
1. 定时将冷数据迁移到Cold存储系统
2. 多个存储系统的实现，统一cache层
3. 向上游屏蔽存储引擎，提供友好的接口


#### 1.6 消息存储设计

功能|消息存储库|消息同步库
---|---|---
数据模型|TimeLine模型|TimeLine模型
读能力|支持高并发读 10W级别TPS|支持高并发写 1W级别TPS
写能力|支持高并发写 10W级别TPS|支持高并发写 1W级别TPS
存储能力|保留一段时间内的消息|保留全量TB级别历史记录
库选型|Redis|MySQL or Hbase

实现消息漫游和多端同步功能，基于双库存储库和同步库来实现

### 2. 协议设计

#### 2.1 应用层协议设计
定长的包头和边长的包体

### 3. Session设计和管理


### 4. 安全管理
安全层协议设计
基于动态密钥，借鉴类似SSL，不需要用证书来管理，原理如下
1. 客户端请求服务端生成安全通道
2. 服务端生成非对称加密的公私钥(公钥A1),将A1传给客户端
3. 客户端本地生成公私钥A2， 利用A1加密A2生成密文传给服务端
4. 服务端利用A1解密密文，拿到A2公钥
5. 服务端随机生成对称加密密钥，传给客户端
6. 后续的内容传输都用对称加密进行通信
注：对称加密传输内容，非对称加密传输密钥


### 5. 消息管理
#### 消息可靠性如何保证 不丢消息


#### 消息顺序性如何保证 不乱序


#### 消息重复性如何保证 不重复

#### 消息实时性同步如何实现
1. Timeline模型特征
 - 每个消息拥有一个顺序ID（SeqId），在队列后面的消息的SeqId一定比前面的消息的SeqId大，也就是保证SeqId一定是增长的；
 - 新的消息永远在尾部添加，保证新的消息的SeqId永远比已经存在队列中的消息都大；
 - 可根据SeqId随机定位到具体的某条消息进行读取，也可以任意读取某个给定范围内的所有消息。
 

2. 消息同步库用于存储所有用于消息同步的Timeline，每个Timeline对应一个接收端，主要用作写扩散模式的消息同步；
消息存储库用于消息漫游时拉取某个会话的所有历史消息，主要用作读扩散模式的消息同步和漫游。

先写入发件箱 Timeline（存储库），后写扩散至各个接收端的收件箱（同步库）。

### 6. 高并发支持
水平扩展：各个模块无状态部署
线程模型：netty主从reactor模型

### 7. 网关设计
#### 7.1 接入层网关设计
##### 7.1.0 目标：
1. 单机能够承载5W的连接 

##### 7.1.1 设计方案：

#### 7.2 应用层API网关设计
##### 7.2.0 目标：
1. 降成本提效率
 - 提供管理端界面，标准化其他前台业务和应用服务
 - 开发只需要关注service层业务编写，不需要关注网关层的HTTP协议
 - 灰度/扩容 基于版本的自动发现，不需要关注IP（基于版本的扩容和灰度）
 - 对前台应用提供标准化扩展能力
2. 保证质量
 - 灰度能力保证迭代上线版本风险低
 - 所有接口的名字和参数以及版本都能一目了然，提供细粒度管理
 
##### 7.2.1 设计方案：

### 8. 业务优化

#### 8.0 性能优化
1. 对消息的查询处理：
 - 短期消息(7天)：存储在Redis里；
 - 近期消息(1-3个月)：存储在Mysql里，以备用户实时查询；
 - 历史信息：存储在HBase里，作为历史数据慢查询

2. 对消息的写入处理：
 - 对并发不高的场景直接rpc调用
 - 对高并发场景可以引入Kafka
 - 写入量巨大时，水平切分能够扩容，MQ缓冲可以保护数据库
 - 需要考虑到多个存储端的事务一致性问题
 
#### 8.1 LSB优化
问题背景：当某个实例重启后，该实例的连接断开后，客户端会发起重连，重连就大概率转移其他实例上，导致最近启动的实例连接数较少，最早启动的实例连接数较多

解决方法：
1、客户端会发起重连，跟服务器申请重连的新的服务器IP，系统提供合适的算法来平摊gate层的压力，防止雪崩效应。

2、gate层定时上报本机的元数据信息以及连接数信息，提供给LSB中心，LSB根据最少连接数负载均衡实现，来计算一个节点供连接。

#### 8.2 群组优化
群组优化
1. 批量消息处理
2. redis扩散写，MySQL扩散读
3. 群消息和成员批量加载

#### 8.3 gate缓存优化
gate一级缓存，redis二级缓存.如何维护缓存一致性？可以参考MQ的广播消息

#### 8.4 消息的存储以及扩容

#### 8.5 snowflake算法的唯一ID生成
##### 8.5.0 缺点：
1. 时间靠的近的消息，区分不出来消息的先后顺序
2. 当并发度不高的时候，生成的ID后面的最后一位是0，导致数据倾斜

##### 8.5.1 解决方案
去掉snowflake最后8位，然后对剩余的位进行取模

#### 8.6 存储库的选型

1. 前期消息不多的场景可以使用MySQL进行分库分表，当系统瓶颈在数据库的时候，就要进行额外的选型了
2. 分析IM消息特征
 - 根据消息ID查某一条消息，要么根据一个消息要检索这个范围之间的消息，不需要强的事务型数据库
 - 当数据量大的时候，如果对关系型数据库比如MySQL进行分库分表，运维成本加上开发成本会非常高
3. 可考虑NoSQL 方案
1）K-V 存储2）文档数据库3）列式数据库4）全文搜索引擎

### 9. 心跳设计
1. 服务端检测到某个客户端迟迟没有心跳过来可以主动关闭通道，让它下线；
2. 客户端检测到某个服务端迟迟没有响应心跳也能重连获取一个新的连接。


## 二、技术选型
1. 配置中心 Nacos2.0
2. 注册中心 Nacos2.0
2. 服务端语言和框架 Java + Spring + Netty
3. 打包框架 Gradle
4. 消息队列 Kafka

## 三、Function
### TODO 
#### 优先级I
1. 网关设计
2. 聊天记录存储和查询
3. 客户端自动重连
4. 群聊
5. 登录、注册、登出实现
6. 消息推送、数据上报
7. 提供客户端jar
8. 多端同步消息和消息漫游
9. 单聊


#### 优先级II
1. 唯一设备踢出
2. 文件发送，图片发送
3. 消息安全：聊天加密
4. 群红包
5. 消息撤回
6. 已读未读

### Finish
1. IM架构设计
2. 分布式ID设计
3. 心跳保活设计

## 四、性能测试

## 五、Q&A
### 1 架构
2. 消息ID为什么是趋势递增就可以，为什么不是严格递增的？
3. 架构设计为什么要增加router层？为什么不当Kafka作为消息总线来进行gate层和logic层解耦？
4. 为何消息存储存两份数据，一份到存储库，一份到消息同步库？
5. Q:为什么接入层用LSB返回的IP来做接入呢？
   A: 1、灵活的负载均衡策略 可根据最少连接数来分配IP；2、做灰度策略来分配IP；3、AppId业务隔离策略 不同业务连接不同的gate，防止相互影响
6. Q:为什么应用层心跳对连接进行健康检查？
   A:因为TCP Keepalive状态无法反应应用层状态问题，如进程阻塞、死锁、TCP缓冲区满等情况；并且要注意心跳的频率，频率小则可能及时感知不到应用情况，频率大可能有一定的性能开销。

### 2 技术细节
1. Q:本地写数据成功，一定代表对端应用侧接收读取消息了吗？
   A:本地TCP写操作成功，但数据可能还在本地写缓冲区中、网络链路设备中、对端读缓冲区中，并不代表对端应用读取到了数据
   
2. 为什么写同步消息到redis后，然后进行推送通知让客户端主动来拉最新的数据？
